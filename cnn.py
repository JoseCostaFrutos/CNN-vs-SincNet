# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yffm-a-UBTG0mSs54k3a75KCnZBaVx0S

# <center>Freesound General-Purpose Audio Tagging Challenge</center>

![Logo](https://upload.wikimedia.org/wikipedia/commons/3/3c/Freesound_project_website_logo.png)

Freesound is a collaborative database of Creative Commons Licensed sounds. The aim of this competition is to classify audio files that cover real-world sounds from musical instruments, humans, animals, machines, etc. Few of the labels are: `Trumpet`, `Squeak`, `Meow`, `Applause` and `Finger_sapping`.  One of the challenges is that not all labels are manually verified. 

<a id="eda"></a>
## <center>1. Exploratory Data Analysis</center>
"""

# Change this to True to replicate the result
# COMPLETE_RUN = False
COMPLETE_RUN = True

"""## Downloading data from a Shared Google Drive zip file

        
* The file audio_train8.zip contains original training FreeSound Challenge data <font color=red> downsampled to 8 KHz</font> using subsample.py

* File train.csv contains the labelling information provided by FreeSounf
"""

! pip install googledrivedownloader

from google_drive_downloader import GoogleDriveDownloader as gdd



# Download csv data from a shared GoogleDrive file

gdd.download_file_from_google_drive(file_id='1wEUNo9A_2W29YD8HWGNPoshs6Irk4VzQ',
                                    dest_path='./train.csv',
                                    unzip=False)


# Download audio data from a shared GoogleDrive file

gdd.download_file_from_google_drive(file_id='19c_Pc9dC_E96AGxN5ZdB-5UeOCDpYPvW',
                                    dest_path='./audio_train8.zip',
                                    unzip=False)

ls

"""## Extract audio (wav) files into audio_train8 subdirectory"""

! unzip -q -o audio_train8.zip

"""## Check that there are 9473 audio files"""

ls ./audio_train8 | wc

"""## First some imports"""

import plotly.plotly as py
import plotly.graph_objs as go
import numpy as np
np.random.seed(1001)

import os
import shutil

import IPython
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from tqdm import tqdm_notebook as tqdm

# %matplotlib inline
matplotlib.style.use('ggplot')

"""## Read 'train.csv' file"""

train = pd.read_csv("train.csv")

train.head()

"""## Check that there is no empty field in the field"""

total = train.isnull().sum().sort_values(ascending = False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending = False)
missing_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_train.head()

"""## Number of audio fields and number of classes"""

print("Number of training examples=", train.shape[0], "  Number of classes=", len(train.label.unique()))

"""## Name of the classes"""

print("Labels are : ",train.label.unique())

"""## Manually verification of labels"""

temp = train['manually_verified'].value_counts()
labels = temp.index
sizes = (temp / temp.sum())*100

fig1, ax1 = plt.subplots()
s,l,t=ax1.pie(sizes, labels=labels, autopct='%1.1f%%',
        shadow=False, startangle=90,textprops=dict(color="w"))
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
ax1.set_title("Manually varification of labels (0 - No, 1 - Yes)")
plt.setp(t, size=15)
ax1.legend()
plt.show()

"""## Number of audio files (verified and non-verified) per class"""

category_group = train.groupby(['label', 'manually_verified']).count()
plot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\
          .plot(kind='bar', stacked=True, title="Number of Audio Samples per Category",figsize=(15,6))
plt.xticks(rotation=80, fontsize=17)
plt.yticks(fontsize=17)
plot.set_xlabel("Category",fontsize=17)
plot.set_ylabel("Number of Samples",fontsize=17);

print('Minimum samples per category = ', min(train.label.value_counts()))
print('Maximum samples per category = ', max(train.label.value_counts()))

"""<a id="audio_files"></a>
## Reading Audio Files

The audios are [Pulse-code modulated](https://en.wikipedia.org/wiki/Audio_bit_depth) with a [bit depth](https://en.wikipedia.org/wiki/Audio_bit_depth) of 16 and a [sampling rate](https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29) of 8 kHz (NOT 44.1 kHz)

![16-bit PCM](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Pcm.svg/500px-Pcm.svg.png)

* **Bit-depth = 16**: The amplitude of each sample in the audio is one of 2^16 (=65536) possible values. 
* **Samplig rate = 8000 (NOT 44.1 kHz)**: Each second in the audio consists of **8000** not 44100 samples. So, if the duration of the audio file is 3.2 seconds, the audio will consist of 8000\*3.2= 24000 values (44100\*3.2 = 141120 values).

Let's listen to an audio file in our dataset
"""

import IPython.display as ipd  # To play sound in the notebook
#fname = './audio_train8/' + '00353774.wav'   # Cello
fname = './audio_train8/' + '00044347.wav' #Hi-Hat
#fname = './audio_train8/' + '3d3b9761.wav'

ipd.Audio(fname)

"""## Number of frames and duration of a file"""

# Using wave library
import wave
from scipy.fftpack import fft
from scipy import signal
wav = wave.open(fname)
print("Sampling (frame) rate = ", wav.getframerate())
print("Total samples (frames) = ", wav.getnframes())
print("Duration = ", wav.getnframes()/wav.getframerate())

# Using scipy
from scipy.io import wavfile
rate, data = wavfile.read(fname)
print("Sampling (frame) rate = ", rate)
print("Total samples (frames) = ", data.shape)

"""## Plot a file"""

fig=plt.figure(figsize=(17,10))
ax1 = fig.add_subplot(211)
ax1.plot(data, '-', );
ax1.set_xlabel("nframes",fontsize=17)
ax1.set_ylabel("amplitude",fontsize=17);

"""## Zoom first 2000 samples"""

plt.figure(figsize=(16, 4))
plt.plot(data[:2000], '.'); plt.plot(data[:40000], '-');

"""## Spectrogram of a file"""

def log_specgram(audio, rate, window_size=20,
                 step_size=10, eps=1e-10):
    nperseg = int(round(window_size * rate / 1e3))
    noverlap = int(round(step_size * rate / 1e3))
    freqs, times, spec = signal.spectrogram(audio,
                                    fs=rate,
                                    window='hann',
                                    nperseg=nperseg,
                                    noverlap=noverlap,
                                    detrend=False)
    return freqs, times, np.log(spec.T.astype(np.float32) + eps)

freqs, times, spectrogram = log_specgram(data, rate)

fig = plt.figure(figsize=(18, 8))
ax2 = fig.add_subplot(211)
ax2.imshow(spectrogram.T, aspect='auto', origin='lower', 
           extent=[times.min(), times.max(), freqs.min(), freqs.max()])
ax2.set_yticks(freqs[::40])
ax2.set_xticks(times[::40])
ax2.set_title('Spectrogram of Hi-hat ' + fname)
ax2.set_ylabel('Freqs in Hz')
ax2.set_xlabel('Seconds')

mean = np.mean(spectrogram, axis=0)
std = np.std(spectrogram, axis=0)
spectrogram = (spectrogram - mean) / std

"""## Distribution of audio frames per class"""

train['nframes'] = train['fname'].apply(lambda f: wave.open('./audio_train8/' + f).getnframes())
plt.figure(figsize=(15,6))
boxplot = sns.boxplot(x="label", y="nframes", data=train)
boxplot.set(xlabel='', ylabel='')
plt.title('Distribution of audio frames, per label', fontsize=15)
plt.xticks(rotation=80, fontsize=15)
plt.yticks(fontsize=15)
plt.xlabel('Label name',fontsize=15)
plt.ylabel('nframes',fontsize=15)
plt.show()

"""## Number of files with less than 100.000 frames"""

ar=train['nframes']<100000
ar.value_counts()

"""## Length of the files"""

train.nframes.hist(bins=100)
plt.suptitle('Frame Length Distribution', ha='center', fontsize='large');
plt.xlabel('Number of frames',fontsize=15)
plt.ylabel('Number of audio files',fontsize=15)

"""## Add an index to each label"""

LABELS = list(train.label.unique())
label_idx = {label: i for i, label in enumerate(LABELS)}
train.set_index("fname", inplace=True)
train["label_idx"] = train.label.apply(lambda x: label_idx[x])
if not COMPLETE_RUN:
    train = train[:2000]

train.head()

"""---

## Prepare balanced test data:
"""

# Select a test sample from TRAIN dataset with same number of audio per class

n_samples_per_class=20
test_sample=train.groupby('label')['label_idx'].apply(lambda x: x.sample(n=n_samples_per_class))

# test_sample is a Multilevel panadas Series with the file names in the second level (1)


test_list= list(test_sample.index.get_level_values(1))
test_df=train.loc[train.index.isin(test_list)]

"""---

## Now let's take the rest of data as the Training dataset:
"""

train_df=train.loc[~train.index.isin(test_list)]

print('Number of audios for final train: ',len(train_df))
print('Number of audios for final test : ',len(test_df))

print(train_df)

"""## Configuration

The Configuration object stores those learning parameters that are shared between data generators, models, and training functions. Anything that is `global` as far as the training is concerned can become the part of Configuration object.
"""

class Config(object): #Python class
    def __init__(self,
                 sampling_rate=8000, audio_duration=2, n_classes=41,
                 use_mfcc=False, n_folds=10, learning_rate= 0.001, 
                 max_epochs=50, n_mfcc=20):
        self.sampling_rate = sampling_rate
        self.audio_duration = audio_duration
        self.n_classes = n_classes
        self.use_mfcc = use_mfcc
        self.n_mfcc = n_mfcc
        self.n_folds = n_folds
        self.learning_rate = learning_rate
        self.max_epochs = max_epochs

        self.audio_length = self.sampling_rate * self.audio_duration
        if self.use_mfcc:
            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)
        else:
            self.dim = (self.audio_length, 1)

config = Config(sampling_rate=8000, audio_duration=2, n_folds=10, learning_rate= 0.001)
if not COMPLETE_RUN:
    config = Config(sampling_rate=8000, audio_duration=2, n_folds=2, learning_rate= 0.001, max_epochs=1)

"""## Librosa installation"""

! pip install -q librosa

import librosa

"""## We define a prepare data method to read all the data at once"""

def prepare_data(df, config, data_dir):
    if config.use_mfcc :
      X = np.empty(shape=(df.shape[0], int(config.dim[0]/2), config.dim[1], 1))
    else:
      X = np.empty(shape=(df.shape[0], int(config.dim[0]/2), config.dim[1]))
      
    input_length = int(config.audio_length/2)
    for i, fname in enumerate(df.index):
        
        file_path = data_dir + fname
        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type="kaiser_fast")

        # Random offset / Padding
        if len(data) > input_length:
            max_offset = len(data) - input_length
            offset = np.random.randint(max_offset)
            data = data[offset:(input_length+offset)]
        else:
            if input_length > len(data):
                max_offset = input_length - len(data)
                offset = np.random.randint(max_offset)
            else:
                offset = 0
            data = np.pad(data, (offset, input_length - len(data) - offset), "constant")

        if config.use_mfcc :
          data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)
          data = np.expand_dims(data, axis=-1)
          
        else:
          data = data[:, np.newaxis]
          
            
        X[i,] = data
    return X

X_test = prepare_data(test_df, config, './audio_train8/')

X_test.shape

X_train = prepare_data(train_df, config, './audio_train8/')

X_train.shape

"""## Labels: we will use one-hot-encoding from to_categorical provided by Keras"""

from tensorflow import keras
from keras.utils import to_categorical

#when using the categorical_crossentropy loss, your targets should be in categorical format 
#e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector. 
#In order to convert integer targets into categorical targets, you can use the Keras utility to_categorical:
test_labels = to_categorical(list(test_df.label_idx), num_classes=config.n_classes)
train_labels = to_categorical(list(train_df.label_idx), num_classes=config.n_classes)

print('Train labels shape',train_labels.shape)
print('Test labels shape',test_labels.shape)

"""<a id="1d_model_building"></a>
## <center>3. Building a Neural Network Model</center>

## Some essential imports
"""

from keras import losses, models, optimizers
from keras.activations import relu, softmax
from keras.callbacks import (EarlyStopping, LearningRateScheduler,
                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)
from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, 
                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)
from keras.utils import Sequence, to_categorical

"""<a id="1d_normalization"></a>
### Normalization
"""

def audio_norm(data):
    max_data = np.max(data)
    min_data = np.min(data)
    data = (data-min_data)/(max_data-min_data+1e-6)
    return data-0.5

"""## CNN Architecture"""

def get_1d_dummy_model(config):
    
    nclass = config.n_classes
    input_length = int(config.audio_length/2)
    
    inp = Input(shape=(input_length,1))
    x = GlobalMaxPool1D()(inp)
    out = Dense(nclass, activation=softmax)(x)

    model = models.Model(inputs=inp, outputs=out)
    opt = optimizers.Adam(config.learning_rate)

    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])
    return model

def get_1d_conv_model(config):
    
    nclass = config.n_classes
    input_length = int(config.audio_length/2)
    
    inp = Input(shape=(input_length,1))
    x = Convolution1D(16, 9, activation=relu, padding="valid")(inp)
    x = Convolution1D(16, 9, activation=relu, padding="valid")(x)
    x = MaxPool1D(16)(x)
    x = Dropout(rate=0.1)(x)
    
    x = Convolution1D(32, 3, activation=relu, padding="valid")(x)
    x = Convolution1D(32, 3, activation=relu, padding="valid")(x)
    x = MaxPool1D(4)(x)
    x = Dropout(rate=0.1)(x)
    
    x = Convolution1D(32, 3, activation=relu, padding="valid")(x)
    x = Convolution1D(32, 3, activation=relu, padding="valid")(x)
    x = MaxPool1D(4)(x)
    x = Dropout(rate=0.1)(x)
    
    x = Convolution1D(256, 3, activation=relu, padding="valid")(x)
    x = Convolution1D(256, 3, activation=relu, padding="valid")(x)
    x = GlobalMaxPool1D()(x)
    x = Dropout(rate=0.2)(x)

    x = Dense(64, activation=relu)(x)
    x = Dense(1028, activation=relu)(x)
    out = Dense(nclass, activation=softmax)(x)

    model = models.Model(inputs=inp, outputs=out)
    rmsprop = optimizers.RMSprop(config.learning_rate)
    opt = optimizers.Adam(config.learning_rate)
    sgd = optimizers.SGD(config.learning_rate)
    adagrad = optimizers.Adagrad(config.learning_rate)

    model.compile(optimizer=rmsprop, loss=losses.categorical_crossentropy, metrics=['acc'])
    return model

if COMPLETE_RUN:
        model = get_1d_conv_model(config)
else:
        model = get_1d_dummy_model(config)

model.summary()

history = model.fit(X_train, train_labels, batch_size=100, epochs=300 , \
                    shuffle=True, validation_data=(X_test, test_labels))

"""## Filters obtained"""

#Note that weights and biases are already numpy arrays.

weights = model.layers[1].get_weights()[0]
bias = model.layers[1].get_weights()[1]

import numpy as np
import scipy as sp
import scipy.signal as sig


i = 0
filt_i=weights[:,0,i]

w, h = sig.freqz(filt_i,1,512)

plt.plot((w/np.pi)*4000,np.log10(np.abs(h)), 'b')
plt.ylabel('Amplitude (dB)', color='b')
plt.xlabel('Frequency: in DFT points')

# Initialise the figure and a subplot axes. Each subplot sharing (showing) the
# same range of values for the x and y axis in the plots.
fig, axes = plt.subplots(4, 4, figsize=(12, 8), sharex=False, sharey=True)

kfilt=0
for i in range(4):
  for j in range(4):
    filt_k=weights[:,0,kfilt]
    w, h = sig.freqz(filt_k,1,256)
    axes[i,j].plot((w/np.pi)*4000,np.log10(np.abs(h)), 'b')
    kfilt = kfilt + 1

plt.show()

"""## Metrics"""

# Check accuracy on train set

loss_train, accuracy_train = model.evaluate(X_train, train_labels, batch_size=100)



# Check accuracy on test set

loss, accuracy = model.evaluate(X_test, test_labels, batch_size=100)


print('Train Accuracy: ',np.round(history.history['acc'][-1],2))
print('Test_Accuracy: ',np.round(history.history['val_acc'][-1],2))
print('Train Loss: ',np.round(history.history['loss'][-1],2))
print('Test_Loss: ',np.round(history.history['val_loss'][-1],2))

# Plot the accuracy curves
plt.plot(history.history['acc'],'bo')
plt.plot(history.history['val_acc'],'rX')

plt.plot(history.history['loss'],'bo')
plt.plot(history.history['val_loss'],'rX')

"""## Apply the divided_data in the model"""

# First we'll define a function that normalizes the data to be between -1 and 1, as opposed to -32768 and 32768.

def normalize_data(data):
    # audio = (audio + 32768) / 65535 (only if bits were correct)
    data = data / max(np.abs(data))
    return data

# We'll eliminate noise from the tracks by measuring the volume in each segment and eliminating very quiet segments.

# This function will return the start and stop of the signal parts of each track.

def divide_data(data, resolution=100, window_duration=0.1, minimum_power=0.001, sample_rate=8000):    
    duration = len(data) / sample_rate # in samples/sec
    iterations = int(duration * resolution)
    step = int(sample_rate / resolution)
    window_length = np.floor(sample_rate * window_duration)
    data_power = np.square(normalize_data(data)) / window_length #Normalized power to window duration

    start = np.array([])
    stop = np.array([])
    is_started = False
    
    for n in range(iterations):
        power = 10 * np.sum(data_power[n * step : int(n * step + window_length)]) # sensitive
        if not is_started and power > minimum_power:
            start = np.append(start, n * step + window_length / 2)
            is_started = True
        elif is_started and (power <= minimum_power or n == iterations-1):
            stop = np.append(stop, n * step + window_length / 2)
            is_started = False
    
    if start.size == 0:
        start = np.append(start, 0)
        stop = np.append(stop, len(data))
        
    start = start.astype(int)
    stop = stop.astype(int)
    
    # We don't want to eliminate EVERYTHING that's unnecessary
    # There should be a little boundary...
    # 200 frame buffer before and after
     # minus = ?
    if start[0] > 200:
        minus = 200
    else:
        minus = start[0]
        
    # plus = ?
    if (len(data) - stop[0]) > 200:
        plus = 200
    else:
        plus = len(data) - stop[0]
    
    return (start - minus), (stop + plus)

"""## Table with file name,  original duration, number of segments and new duration"""

train_ids = next(os.walk('./audio_train8/'))[2]
test_ids = next(os.walk('./audio_train8/'))[2]
columns = ['File Name', 'Audio Duration', 'Segment Number']
audio_segments = pd.DataFrame(columns=columns)

fig, ax = plt.subplots(10, 4, figsize = (12, 16))
for i in tqdm(range(40), total=40):
    random_audio_idxs = np.random.randint(len(train_ids)+1, size=1)[0]
    _, tmp = wavfile.read('./audio_train8/' + train_ids[random_audio_idxs])
    start, stop = divide_data(tmp)
    
    audio_segments = audio_segments.append({'File Name': train_ids[random_audio_idxs],
                                            'Audio Duration': len(tmp)/rate,
                                            'Audio Duration Segmented': len(tmp[start[0]:stop[0]])/rate,
                                            'Segment Number': len(start)}, ignore_index=True)
    
    
    ax[i//4, i%4].plot(tmp[start[0]:stop[0]]);
    ax[i//4, i%4].set_title(train_ids[random_audio_idxs])
    ax[i//4, i%4].get_xaxis().set_ticks([])

audio_segments

def prepare_data2(df, config, data_dir):
    if config.use_mfcc :
      X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))
    else:
      X = np.empty(shape=(df.shape[0], int(config.dim[0]/2), config.dim[1]))
      
    input_length = int(config.audio_length/2)
    for i, fname in enumerate(df.index):
        
        file_path = data_dir + fname
        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type="kaiser_fast")

        # Random offset / Padding
        start, stop = divide_data(data)
        data = data[start[0]:stop[0]]
        if len(data) > input_length:
            max_offset = len(data) - input_length
            offset = np.random.randint(max_offset)
            data = data[offset:(input_length+offset)]
        else:
            if input_length > len(data):
                max_offset = input_length - len(data)
                offset = np.random.randint(max_offset)
            else:
                offset = 0
            data = np.pad(data, (offset, input_length - len(data) - offset), "constant")

        if config.use_mfcc :
          data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)
          data = np.expand_dims(data, axis=-1)
          
        else:
          data = data[:, np.newaxis]
          
            
        X[i,] = data
    return X

X_test = prepare_data2(test_df, config, './audio_train8/')

X_train = prepare_data2(train_df, config, './audio_train8/')

history = model.fit(X_train, train_labels, batch_size=100, epochs=300 , \
                    shuffle=True, validation_data=(X_test, test_labels))

# Check accuracy on train set

loss_train, accuracy_train = model.evaluate(X_train, train_labels, batch_size=100)



# Check accuracy on test set

loss, accuracy = model.evaluate(X_test, test_labels, batch_size=100)


print('Train Accuracy: ',np.round(history.history['acc'][-1],2))
print('Test_Accuracy: ',np.round(history.history['val_acc'][-1],2))
print('Train Loss: ',np.round(history.history['loss'][-1],2))
print('Test_Loss: ',np.round(history.history['val_loss'][-1],2))

# Plot the accuracy curves
plt.plot(history.history['acc'],'bo')
plt.plot(history.history['val_acc'],'rX')

plt.plot(history.history['loss'],'bo')
plt.plot(history.history['val_loss'],'rX')

"""## We create another function to improve the accuracy"""

# This function provide the point of maximum power of the audio file
def divide_data2(data, max_power=0.001):    
    iterations = len(data)   
    maximum=1
    for n in range(iterations):
        power = np.abs(data[n]) # sensitive
        if power > max_power:
            max_power = power
            maximum = n
    
    return maximum

# Now to select the audio length we create a window around the point of max power

def prepare_data3(df, config, data_dir):
    if config.use_mfcc :
      X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))
    else:
      X = np.empty(shape=(df.shape[0], int(config.dim[0]/2), config.dim[1]))
      
      
    input_length = config.audio_length
    for i, fname in enumerate(df.index):
       print(i)
        file_path = data_dir + fname
        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type="kaiser_fast")
        if len(data) > input_length/2:
        # Random offset / Padding
          maximum = divide_data2(data)
          begtotal = int(maximum - input_length/2)
          beg = int(maximum - input_length/4)
          end =int (maximum + input_length/4)
          endtotal =int (maximum + input_length/2)
          if beg> 0 and end <len(data):
            data = data[beg:end]
          else:
            if begtotal> 0:
                data = data[maximum-int(input_length/2):maximum]
            else:
              if endtotal <len(data):
                data = data[maximum:maximum+int(input_length/2)] 
              else:
                data = data[0:int(input_length/2)]
        else: 
          if input_length/2 > len(data):
            data = np.pad(data,(abs(int(input_length/2)-len(data)),0),"constant")   
          else:
            data = np.pad(data,int(input_length/2)-len(data),"constant")
        if config.use_mfcc :
          data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)
          data = np.expand_dims(data, axis=-1)
        else:
          data = data[:, np.newaxis]
          

        X[int(i),] = data
    return X

X_test = prepare_data3(test_df, config, './audio_train8/')

X_train = prepare_data3(train_df, config, './audio_train8/')

history = model.fit(X_train, train_labels, batch_size=100, epochs=300 , \
                    shuffle=True, validation_data=(X_test, test_labels))

# Check accuracy on train set

loss_train, accuracy_train = model.evaluate(X_train, train_labels, batch_size=100)



# Check accuracy on test set

loss, accuracy = model.evaluate(X_test, test_labels, batch_size=100)


print('Train Accuracy: ',np.round(history.history['acc'][-1],2))
print('Test_Accuracy: ',np.round(history.history['val_acc'][-1],2))
print('Train Loss: ',np.round(history.history['loss'][-1],2))
print('Test_Loss: ',np.round(history.history['val_loss'][-1],2))

# Plot the accuracy curves
plt.plot(history.history['acc'],'bo')
plt.plot(history.history['val_acc'],'rX')